{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070faf27-4f77-49c2-9a4f-0a3931e69c29",
   "metadata": {},
   "source": [
    "# ogbn-papers100M benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c9e93d-d680-410e-b1fe-a8fefa6c699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RAPIDS_NO_INITIALIZE=1\n"
     ]
    }
   ],
   "source": [
    "%env RAPIDS_NO_INITIALIZE=1\n",
    "single_gpu = False\n",
    "use_cugraph = True\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c0c0d5-6b23-47bf-9b83-1c6ad6aeaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if single_gpu:\n",
    "    import os\n",
    "    import warnings\n",
    "    if use_cugraph:\n",
    "        # Enable when Pytorch+RMM lands\n",
    "        warnings.warn(\"Error on 1 GPU even with managed memory, Switch on MULTI-GPU\")\n",
    "        import rmm\n",
    "        rmm.reinitialize(managed_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa2d1ce-5d31-4850-b4ec-8c0f4987bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not single_gpu:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    from dask.distributed import Client\n",
    "    import cugraph.dask.comms.comms as Comms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b78fd-9388-4231-930c-7592cb3a6ad7",
   "metadata": {},
   "source": [
    "## Start Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d3b4a3-53f8-4b2f-bd70-e7b835509db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1671324589.007996] [exp02:82923:0]          parser.c:1989 UCX  WARN  unused environment variable: UCX_MEMTYPE_CACHE (maybe: UCX_MEMTYPE_CACHE?)\n",
      "[1671324589.007996] [exp02:82923:0]          parser.c:1989 UCX  WARN  (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)\n",
      "[1671324589.502557] [exp02:82923:0]            sock.c:470  UCX  ERROR bind(fd=260 addr=0.0.0.0:53867) failed: Address already in use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 16:49:51,987 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-096ke4wg', purging\n",
      "2022-12-17 16:49:51,987 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-vrj5catg', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-ts_7mw3u', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-te7fs10k', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-7yc7no2h', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-0an9buyk', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-lw3ztcxy', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-tsecyucb', purging\n",
      "2022-12-17 16:49:51,988 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-_rx3hj5v', purging\n",
      "2022-12-17 16:49:51,989 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-kdyddcov', purging\n",
      "2022-12-17 16:49:51,989 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-4quibk2w', purging\n",
      "2022-12-17 16:49:51,989 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space-10123/worker-fa_euyph', purging\n",
      "2022-12-17 16:49:51,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:51,989 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,141 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,212 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,212 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,402 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,402 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,532 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,533 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2022-12-17 16:49:52,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    }
   ],
   "source": [
    "if not single_gpu:\n",
    "    cluster = LocalCUDACluster(protocol='ucx',rmm_pool_size='25GB', CUDA_VISIBLE_DEVICES='1,2,3,4,5,6,7,8,9,10,11,12')\n",
    "                               #jit_unspill=False)\n",
    "    client = Client(cluster)\n",
    "    Comms.initialize(p2p=True)\n",
    "if use_cugraph:\n",
    "    import rmm\n",
    "    rmm.reinitialize(pool_allocator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a048887-b998-4ed5-9715-b803665cf8c1",
   "metadata": {},
   "source": [
    "# Import DGL Specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430a2887-993c-423d-b102-b6f164b12f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/vjawa/miniconda3/envs/cugraph-ucx-23.02-dec-17/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "from cugraph_dgl.convert import cugraph_storage_from_heterograph\n",
    "import cudf\n",
    "\n",
    "from dgl.data import AsNodePredDataset\n",
    "import dgl.backend as F\n",
    "from dgl.dataloading import DataLoader, NeighborSampler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976fd76-632a-4c7c-95cd-212f4f24bdad",
   "metadata": {},
   "source": [
    "\n",
    "# Create cugraph_dgl GraphStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54861b1a-fabd-445c-ae84-fb32ecab7111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.97 s, sys: 49.1 s, total: 56 s\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "dataset = DglNodePropPredDataset(\"ogbn-papers100M\", root='/raid/vjawa/gnn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fffc84b-c740-49e9-b450-80a2073d1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.05 s, sys: 8.04 s, total: 15.1 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gs,labels = dataset[0]\n",
    "gs = gs.int()\n",
    "## Clear Data to allow scale testing \n",
    "## Check how to not do this\n",
    "gs.ndata.clear()\n",
    "gs.edata.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d965bb3-9a5c-4e2c-8d0e-8465ef14fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.9 s, sys: 19.6 s, total: 1min 17s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if use_cugraph:\n",
    "    gs = cugraph_storage_from_heterograph(gs, single_gpu=single_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcd4b02-8c44-43eb-927e-ec727a80fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Train IDS\n",
    "# train_ids = gs.graphstore.gdata.get_edge_data()['_DST_'].unique()\n",
    "# if hasattr(train_ids, 'compute'):\n",
    "#     train_ids=train_ids.compute()\n",
    "# train_ids.to_frame().to_parquet('train_nids.parquet')\n",
    "train_ids = cudf.read_parquet('train_nids.parquet')['_DST_']\n",
    "train_ids = torch.as_tensor(train_ids.values).to(gs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549e87bf-9aad-4c59-a122-36cef4befb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'89,566,794'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{len(train_ids):,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1deeca7-7243-4537-bd4f-13247e62e5d1",
   "metadata": {},
   "source": [
    "## Benchmark Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9391e4-4936-4caa-b245-4c05e24cc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_loader(dataloader):\n",
    "    for it,(input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        break\n",
    "    return\n",
    "\n",
    "def benchmark_dataloader(g,train_ids,batch_size):\n",
    "    batch_size = batch_size\n",
    "    sampler = NeighborSampler(\n",
    "            [10, 10],  # fanout for [layer-0, layer-1]\n",
    "            #prefetch_node_feats=[\"feat\"],\n",
    "            #prefetch_labels=[\"label\"],\n",
    "        )\n",
    "    dataloader = DataLoader(\n",
    "        g,\n",
    "        train_ids[:2_000_000],\n",
    "        sampler,\n",
    "        device='cuda',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    setup_loader(dataloader)\n",
    "    n_nodes = len(dataloader.indices)\n",
    "    st = time.time()\n",
    "    for it,(input_nodes, output_nodes, blocks) in enumerate(dataloader):\n",
    "        input_nodes\n",
    "    et = time.time()\n",
    "    print(f\"Completed sampling {n_nodes:,}  nodes with batch_size {batch_size:,} in  {et-st} s time on graph type {type(gs)}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b281acd7-5254-4a78-8852-1fac2f2b47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(gs.graphstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fc75994-61b8-4bdd-b677-32d4878672d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed sampling 2,000,000  nodes with batch_size 10,000 in  52.02749013900757 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 20,000 in  25.835906982421875 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 40,000 in  13.072922468185425 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 80,000 in  7.277414560317993 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 80,000 in  6.9900617599487305 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 160,000 in  3.862619161605835 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
      "Completed sampling 2,000,000  nodes with batch_size 320,000 in  2.3878278732299805 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [10_000, 20_000, 40_000, 80_000, 80_000, 160_000, 320_000]:\n",
    "    benchmark_dataloader(gs, train_ids, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863f4cb-1641-4d00-b157-4a5db0d69fc5",
   "metadata": {},
   "source": [
    "# DGL CPU\n",
    "```bash\n",
    "Completed sampling 2,000,000  nodes with 10,000 in  12.739480972290039 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "Completed sampling 2,000,000  nodes with 20,000 in  11.33974838256836 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "Completed sampling 2,000,000  nodes with 40,000 in  11.867838621139526 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "Completed sampling 2,000,000  nodes with 80,000 in  11.385798215866089 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "Completed sampling 2,000,000  nodes with 160,000 in  12.451390027999878 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "Completed sampling 2,000,000  nodes with 320,000 in  13.918638706207275 s time on graph type <class 'dgl.heterograph.DGLHeteroGraph'>\n",
    "```\n",
    "\n",
    "# DGL GPU Fails \n",
    "\n",
    "## GPU (11 GPU and 1 training client)\n",
    "```bash\n",
    "Completed sampling 2,000,000  nodes with batch_size 10,000 in  52.02749013900757 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 20,000 in  25.835906982421875 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 40,000 in  13.072922468185425 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 80,000 in  7.277414560317993 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 80,000 in  6.9900617599487305 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 160,000 in  3.862619161605835 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "Completed sampling 2,000,000  nodes with batch_size 320,000 in  2.3878278732299805 s time on graph type <class 'cugraph_dgl.cugraph_storage.CuGraphStorage'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c409253-ff80-469d-81ff-19899a6065d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nfs/vjawa/dgl/cugraph/benchmarks/cugraph_dgl/sampling/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22032a3-1281-4f9c-adef-45008672899a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
