# cuGraph benchmarks

## Overview

The sources are currently intended to benchmark `cuGraph` via the python API,
but future updates may include benchmarks written in C++ or other languages.

The benchmarks here use datasets generated by the RMAT graph generator but also
support csv files as input.

## Prerequisites
* cugraph built and installed (or `cugraph` sources and built C++ extensions
  available on `PYTHONPATH`)

* Multi-node multi-GPU (MNMG) runs require a cluster environment set up and a
  dask scheduler .json file be made accessible to all workers on every node.

* While not a strict prerequisite, the RAPIDS multi-gpu-tools package can be
  used to automate setting up a cluster for MNMG runs and will be referenced in
  the examples below.

## Single-node multi-GPU runs (SNMG)
* Set the `CUDA_VISIBLE_DEVICES` environment variable to the number of GPUs to
  use for the benchmarks. If the env var is unset, it is assumed to be a
  single-GPU run. _Note: this is a slightly different usage for
  `CUDA_VISIBLE_DEVICES` compared to the standard documented use of this env
  var, where having it unset means "no restricted GPUs, use all of them"._

* Run `python main.py --help` for a list of all available benchmark options

* Run `python main.py` to run individual algo benchmarks with specific
  options. For example, to benchmark a 2-GPU run of BFS and SSSP with a
  generated graph of size scale 23:
```
(rapids) user@machine:/cugraph/benchmarks/python_e2e> export CUDA_VISIBLE_DEVICES=0,1

(rapids) user@machine:/cugraph/benchmarks/python_e2e> python main.py --scale=23 --algo=bfs --algo=sssp
calling setup...distributed.preloading - INFO - Import preload module: dask_cuda.initialize
distributed.preloading - INFO - Import preload module: dask_cuda.initialize
done.
running generate_edgelist (RMAT)...done.
running from_dask_cudf_edgelist...done.
running compute_renumber_edge_list...done.
running compute_renumber_edge_list...done.
running bfs (warmup)...done.
running bfs...done.
running sssp (warmup)...done.
running sssp...done.
from_dask_cudf_edgelist()     0.0133009
------------------------------------------------------------
bfs(start:73496080)           0.569328
------------------------------------------------------------
sssp(start:73496080)          1.48114

calling teardown...done.
```

* See [run_all_nightly_benches.sh](run_all_nightly_benches.sh) for for an
  example of multiple SNMG runs over different scales, gpu configurations and
  edgefactors

## Multi-node multi-GPU runs (MNMG)
* MNMG runs require a cluster of multi-GPU machines (nodes) that can access the
  same files - usually through something like NFS - and have some network
  connectivity for data communication across GPUs on the nodes.  Each node must
  have the same cugraph environment (using a shared conda environment is
  recommended).

*
## Other Examples:
_**NOTE: Some algos require the graph to be symmetrized (Louvain, WCC) or unweighted.**_
* Run all the benchmarks with a generated datasets of scale=23
```
(rapids) user@machine:/cugraph/benchmarks/python_e2e> python main.py --scale=23
```

* Run all the benchmarks with a generated unweighted dataset of scale=23
```
(rapids) user@machine:/cugraph/benchmarks/python_e2e> python main.py --scale=23 --unweighted
```

* Symmetrize the generated dataset of scale=23 and run all the benchmarks
```
(rapids) user@machine:/cugraph/benchmarks/python_e2e> python main.py --scale=23 --symmetric-graph
```

* Create a graph from a csv file an run all the benchmarks
```
(rapids) user@machine:/cugraph/benchmarks/python_e2e> python main.py --csv='karate.csv'
```
