# Copyright (c) 2023-2024, NVIDIA CORPORATION.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .trainers_pyg import PyGTrainer
from models.pyg import CuGraphSAGE
from datasets import Dataset

import torch
import numpy as np

from torch.nn.parallel import DistributedDataParallel as ddp
from torch.distributed.optim import ZeroRedundancyOptimizer

from cugraph.gnn import FeatureStore
from cugraph_pyg.data import CuGraphStore
from cugraph_pyg.loader import BulkSampleLoader

import os
import re


class PyGCuGraphTrainer(PyGTrainer):
    """
    Trainer implementation for cuGraph-PyG that supports
    WholeGraph as a feature store.
    """

    def __init__(
        self,
        dataset: Dataset,
        model: str = "GraphSAGE",
        device: int = 0,
        rank: int = 0,
        world_size: int = 1,
        gpus_per_node: int = 1,
        num_epochs: int = 1,
        sample_dir: str = ".",
        backend: str = "torch",
        **kwargs,
    ):
        """
        Parameters
        ----------
        dataset: Dataset
            The dataset to train on.
        model: str
            The model to use for training.
            Currently only "GraphSAGE" is supported.
        device: int, default=0
            The CUDA device to use.
        rank: int, default=0
            The global rank of the worker this trainer is assigned to.
        world_size: int, default=1
            The number of workers in the world.
        num_epochs: int, default=1
            The number of training epochs to run.
        sample_dir: str, default="."
            The directory where samples generated by the bulk sampler
            are stored.
        backend: str, default="torch"
            The feature store backend to be used by the cuGraph Feature Store.
            Defaults to "torch".  Options are "torch" and "wholegraph"
        kwargs
            Keyword arguments to pass to the loader.
        """

        import logging

        logger = logging.getLogger("PyGCuGraphTrainer")
        logger.info("creating trainer")
        self.__data = None
        self.__device = device
        self.__rank = rank
        self.__world_size = world_size
        self.__gpus_per_node = gpus_per_node
        self.__num_epochs = num_epochs
        self.__dataset = dataset
        self.__sample_dir = sample_dir
        self.__loader_kwargs = kwargs
        self.__model = self.get_model(model)
        self.__backend = backend
        self.__optimizer = None
        logger.info("created trainer")

    @property
    def rank(self):
        return self.__rank

    @property
    def model(self):
        return self.__model

    @property
    def dataset(self):
        return self.__dataset

    @property
    def optimizer(self):
        if self.__optimizer is None:
            self.__optimizer = ZeroRedundancyOptimizer(
                self.model.parameters(),
                lr=0.01,
                weight_decay=0.0005,
                optimizer_class=torch.optim.Adam,
            )
        return self.__optimizer

    @property
    def num_epochs(self) -> int:
        return self.__num_epochs

    def get_loader(self, epoch: int = 0, stage="train"):
        import logging

        logger = logging.getLogger("PyGCuGraphTrainer")

        logger.info(f"getting loader for epoch {epoch}, {stage} stage")

        # TODO support online sampling
        if stage == "train":
            path = os.path.join(self.__sample_dir, f"epoch={epoch}", stage, "samples")
        elif stage in ["test", "val"]:
            path = os.path.join(self.__sample_dir, stage, "samples")
        else:
            raise ValueError(f"invalid stage {stage}")

        input_files, num_batches = self.get_input_files(path, epoch=epoch, stage=stage)
        loader = BulkSampleLoader(
            self.data,
            self.data,
            None,  # FIXME get input nodes properly
            directory=path,
            input_files=input_files,
            **self.__loader_kwargs,
        )

        logger.info(f"got loader successfully on rank {self.rank}")
        return loader, num_batches

    @property
    def data(self):
        import logging

        logger = logging.getLogger("PyGCuGraphTrainer")
        logger.info("getting data")

        if self.__data is None:
            if self.__backend == "wholegraph":
                logger.info("using wholegraph backend")
                fs = FeatureStore(
                    backend="wholegraph",
                    wg_type="chunked",
                    wg_location="cpu",
                )
            else:
                fs = FeatureStore(backend=self.__backend)
            num_nodes_dict = {}

            if self.__backend == "wholegraph":
                from pylibwholegraph.torch.initialize import get_global_communicator

                wm_comm = get_global_communicator()
                wm_comm.barrier()

            for node_type, x in self.__dataset.x_dict.items():
                logger.debug(f"getting x for {node_type}")
                fs.add_data(x, node_type, "x")
                num_nodes_dict[node_type] = self.__dataset.num_nodes(node_type)
                if self.__backend == "wholegraph":
                    wm_comm.barrier()

            for node_type, y in self.__dataset.y_dict.items():
                logger.debug(f"getting y for {node_type}")

                if self.__backend == "wholegraph":
                    logger.info("using wholegraph backend")
                    fs.add_data(y, node_type, "y")
                    wm_comm.barrier()
                else:
                    y = y.cuda()
                    y = y.reshape((y.shape[0], 1))
                    fs.add_data(y, node_type, "y")

            """
            for node_type, train in self.__dataset.train_dict.items():
                logger.debug(f"getting train for {node_type}")
                train = train.reshape((train.shape[0], 1))
                if self.__backend != "wholegraph":
                    train = train.cuda()
                fs.add_data(train, node_type, "train")

            for node_type, test in self.__dataset.test_dict.items():
                logger.debug(f"getting test for {node_type}")
                test = test.reshape((test.shape[0], 1))
                if self.__backend != "wholegraph":
                    test = test.cuda()
                fs.add_data(test, node_type, "test")

            for node_type, val in self.__dataset.val_dict.items():
                logger.debug(f"getting val for {node_type}")
                val = val.reshape((val.shape[0], 1))
                if self.__backend != "wholegraph":
                    val = val.cuda()
                fs.add_data(val, node_type, "val")
            """

            # TODO support online sampling if the edge index is provided
            num_edges_dict = self.__dataset.edge_index_dict
            if not isinstance(list(num_edges_dict.values())[0], int):
                num_edges_dict = {k: len(v) for k, v in num_edges_dict}

            if self.__backend == "wholegraph":
                wm_comm.barrier()

            self.__data = CuGraphStore(
                fs,
                num_edges_dict,
                num_nodes_dict,
            )

        logger.info(f"got data successfully on rank {self.rank}")

        return self.__data

    def get_model(self, name="GraphSAGE"):
        import logging

        logger = logging.getLogger("PyGCuGraphTrainer")

        logger.info("Creating model...")

        if name != "GraphSAGE":
            raise ValueError("only GraphSAGE is currently supported")

        logger.info("getting input features...")
        num_input_features = self.__dataset.num_input_features

        logger.info("getting output features...")
        num_output_features = self.__dataset.num_labels

        logger.info("getting num neighbors...")
        num_layers = len(self.__loader_kwargs["num_neighbors"])

        logger.info("Got input features, output features, num neighbors")

        with torch.cuda.device(self.__device):
            logger.info("Constructing CuGraphSAGE model...")
            model = (
                CuGraphSAGE(
                    in_channels=num_input_features,
                    hidden_channels=64,
                    out_channels=num_output_features,
                    num_layers=num_layers,
                )
                .to(torch.float32)
                .to(self.__device)
            )

            logger.info("Parallelizing model with ddp...")
            model = ddp(model, device_ids=[self.__device])

        logger.info("done creating model")

        return model

    def get_input_files(self, path, epoch=0, stage="train"):
        file_list = np.array(os.listdir(path))
        file_list.sort()

        np.random.seed(epoch)
        np.random.shuffle(file_list)

        splits = np.array_split(file_list, self.__gpus_per_node)

        import logging

        logger = logging.getLogger("PyGCuGraphTrainer")

        split = splits[self.__device]
        logger.info(f"rank {self.__rank} input files: {str(split)}")

        ex = re.compile(r"batch=([0-9]+)\-([0-9]+).parquet")
        num_batches = min(
            [
                sum([int(ex.match(fname)[2]) - int(ex.match(fname)[1]) for fname in s])
                for s in splits
            ]
        )
        if num_batches == 0:
            raise ValueError(
                f"Too few batches for training with world size {self.__world_size}"
            )

        return split, num_batches
