{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Graph using Multiple GPU\n",
    "#### Author : Don Acosta\n",
    "\n",
    "In this notebook, we will show how to create and use a Property Graph using multiple GPUs \n",
    "\n",
    "This notebook was developed and tested using RAPIDS 23.02 and CUDA 11.5. Please be aware that your system may be different, and you may need to modify the code or install packages to run the below examples. If you think you have found a bug or an error, please file an issue in [cuGraph](https://github.com/rapidsai/cugraph/issues)\n",
    "\n",
    "\n",
    "CuGraph's multi-GPU features leverage Dask. RAPIDS has other projects based on Dask such as dask-cudf and dask-cuda. These products will also be used in this example. Check out [RAPIDS.ai](https://rapids.ai/) to learn more about these technologies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU Property Graph\n",
    "### Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries. We recommend using the [cugraph_dev](https://github.com/rapidsai/cugraph/tree/branch-23.02/conda/environments) env through conda\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from cugraph.dask.comms import comms as Comms\n",
    "import cugraph.dask as dask_cugraph\n",
    "import cugraph\n",
    "import dask_cudf\n",
    "import time\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_file():\n",
    "\n",
    "    data_dir = '../data/'\n",
    "    if not os.path.exists(data_dir):\n",
    "        print('creating data directory')\n",
    "        os.system('mkdir ../data')\n",
    "\n",
    "    # download the Hollywood dataset\n",
    "    base_url = 'https://rapidsai-data.s3.us-east-2.amazonaws.com/cugraph/benchmark/'\n",
    "    fn = 'hollywood.csv'\n",
    "    comp = '.gz'\n",
    "\n",
    "    if not os.path.isfile(data_dir+fn):\n",
    "        if not os.path.isfile(data_dir+fn+comp):\n",
    "            print(f'Downloading {base_url+fn+comp} to {data_dir+fn+comp}')\n",
    "            urllib.request.urlretrieve(base_url+fn+comp, data_dir+fn+comp)\n",
    "        print(f'Decompressing {data_dir+fn+comp}...')\n",
    "        os.system('gunzip '+data_dir+fn+comp)\n",
    "        print(f'{data_dir+fn+comp} decompressed!')\n",
    "    else:\n",
    "        print(f'Your data file, {data_dir+fn}, already exists')\n",
    "\n",
    "    # File path, assuming Notebook directory\n",
    "    return  (data_dir+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize multi-GPU environment\n",
    "Before we get started, we need to setup a Dask local cluster of workers to execute our work and a client to coordinate and schedule work for that cluster. As we see below, we can initiate a cluster and client using only 3 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 09:06:34,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-01-26 09:06:34,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-01-26 09:06:34,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-01-26 09:06:34,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "Comms.initialize(p2p=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from disk\n",
    "cuGraph depends on cudf for data loading and the initial DataFrame creation. The CSV data file contains an edge list, which represents the connection of a vertex to another. The source to destination pairs is what is known as Coordinate Format (COO). In this test case, the data is just two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cugraph.experimental.datasets import email_Eu_core\n",
    "\n",
    "def get_datasets_mg_df():\n",
    "    _df = email_Eu_core.get_edgelist(fetch=True)\n",
    "    _ddf =dask_cudf.from_cudf(_df,npartions=2)\n",
    "    return _ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start ETL timer\n",
    "# t_start = time.time()\n",
    "\n",
    "# # Helper function to set the reader chunk size to automatically get one partition per GPU\n",
    "# input_data_path = get_data_file()\n",
    "# chunksize = dask_cugraph.get_chunksize(input_data_path)\n",
    "\n",
    "# # Multi-GPU CSV reader\n",
    "# e_list = dask_cudf.read_csv(input_data_path, chunksize = chunksize, delimiter=' ', names=['src', 'dst'], dtype=['int32', 'int32'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m e_list \u001b[39m=\u001b[39m get_datasets_mg_df()\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_datasets_mg_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_datasets_mg_df\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m     _df \u001b[39m=\u001b[39m hollywood_2009\u001b[39m.\u001b[39;49mget_edgelist(fetch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m     _ddf \u001b[39m=\u001b[39mdask_cudf\u001b[39m.\u001b[39mfrom_cudf(_df,npartions\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m _ddf\n",
      "File \u001b[0;32m~/dev_1230/cugraph/python/cugraph/cugraph/experimental/datasets/dataset.py:114\u001b[0m, in \u001b[0;36mDataset.get_edgelist\u001b[0;34m(self, fetch)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m full_path\u001b[39m.\u001b[39mis_file():\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m fetch:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__download_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe datafile \u001b[39m\u001b[39m{\u001b[39;00mfull_path\u001b[39m}\u001b[39;00m\u001b[39m does not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m exist. Try get_edgelist(fetch=True)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m to download the datafile\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\n",
      "File \u001b[0;32m~/dev_1230/cugraph/python/cugraph/cugraph/experimental/datasets/dataset.py:91\u001b[0m, in \u001b[0;36mDataset.__download_csv\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     89\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m\"\u001b[39m\u001b[39mfile_type\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dl_path\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mis_dir():\n\u001b[0;32m---> 91\u001b[0m     df \u001b[39m=\u001b[39m cudf\u001b[39m.\u001b[39;49mread_csv(url)\n\u001b[1;32m     92\u001b[0m     df\u001b[39m.\u001b[39mto_csv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dl_path\u001b[39m.\u001b[39mpath \u001b[39m/\u001b[39m filename, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cudfdev/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/cudfdev/lib/python3.9/site-packages/cudf/io/csv.py:88\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, prefix, mangle_dupe_cols, dtype, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, dayfirst, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, comment, delim_whitespace, byte_range, use_python_file_object, storage_options, bytes_per_thread)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m na_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m is_scalar(na_values):\n\u001b[1;32m     86\u001b[0m     na_values \u001b[39m=\u001b[39m [na_values]\n\u001b[0;32m---> 88\u001b[0m df \u001b[39m=\u001b[39m libcudf\u001b[39m.\u001b[39;49mcsv\u001b[39m.\u001b[39;49mread_csv(\n\u001b[1;32m     89\u001b[0m     filepath_or_buffer,\n\u001b[1;32m     90\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m     91\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m     92\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m     93\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m     94\u001b[0m     header\u001b[39m=\u001b[39;49mheader,\n\u001b[1;32m     95\u001b[0m     mangle_dupe_cols\u001b[39m=\u001b[39;49mmangle_dupe_cols,\n\u001b[1;32m     96\u001b[0m     usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m     97\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m     98\u001b[0m     delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m     99\u001b[0m     delim_whitespace\u001b[39m=\u001b[39;49mdelim_whitespace,\n\u001b[1;32m    100\u001b[0m     skipinitialspace\u001b[39m=\u001b[39;49mskipinitialspace,\n\u001b[1;32m    101\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    102\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    103\u001b[0m     skipfooter\u001b[39m=\u001b[39;49mskipfooter,\n\u001b[1;32m    104\u001b[0m     skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[1;32m    105\u001b[0m     dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[1;32m    106\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    107\u001b[0m     thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[1;32m    108\u001b[0m     decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[1;32m    109\u001b[0m     true_values\u001b[39m=\u001b[39;49mtrue_values,\n\u001b[1;32m    110\u001b[0m     false_values\u001b[39m=\u001b[39;49mfalse_values,\n\u001b[1;32m    111\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[1;32m    112\u001b[0m     byte_range\u001b[39m=\u001b[39;49mbyte_range,\n\u001b[1;32m    113\u001b[0m     skip_blank_lines\u001b[39m=\u001b[39;49mskip_blank_lines,\n\u001b[1;32m    114\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    115\u001b[0m     comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[1;32m    116\u001b[0m     na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[1;32m    117\u001b[0m     keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[1;32m    118\u001b[0m     na_filter\u001b[39m=\u001b[39;49mna_filter,\n\u001b[1;32m    119\u001b[0m     prefix\u001b[39m=\u001b[39;49mprefix,\n\u001b[1;32m    120\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, abc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    124\u001b[0m     \u001b[39m# There exists some dtypes in the result columns that is inferred.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39m# Find them and map them to the default dtypes.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     dtype \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dtype\n",
      "File \u001b[0;32mcsv.pyx:426\u001b[0m, in \u001b[0;36mcudf._lib.csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "e_list = get_datasets_mg_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Multi-GPU Property Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cugraph.experimental import MGPropertyGraph\n",
    "pG = MGPropertyGraph()\n",
    "pG.add_edge_data(e_list,vertex_col_names=[\"src\", \"dst\"])\n",
    "pG.get_num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = cugraph.Graph(directed=False) \n",
    "(louvain_df, modularity) = dask_cugraph.louvain(pG.extract_subgraph(create_using=graph))\n",
    "pG.add_vertex_data(louvain_df, vertex_col_name=\"vertex\")\n",
    "pG.get_vertex_data().compute().sort_index(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down the multi-GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comms.destroy()\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Copyright (c) 2023, NVIDIA CORPORATION.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudfdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587ff963ecd34554a9da41c94362e2baa062d9a57502e220f049e10816826984"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
