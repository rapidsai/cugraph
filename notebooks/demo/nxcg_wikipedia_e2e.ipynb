{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nx-cugraph` Demo - Wikipedia Pagerank\n",
    "\n",
    "This notebook demonstrates a zero code change, end-to-end workflow using `cudf.pandas` and `nx-cugraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these two lines to enable cudf.pandas and nx-cugraph\n",
    "\n",
    "%load_ext cudf.pandas\n",
    "!NETWORKX_BACKEND_PRIORITY=cugraph\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-26 23:32:06--  https://data.rapids.ai/cugraph/datasets/cit-Patents.csv\n",
      "Resolving data.rapids.ai (data.rapids.ai)... 108.139.10.83, 108.139.10.10, 108.139.10.39, ...\n",
      "Connecting to data.rapids.ai (data.rapids.ai)|108.139.10.83|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261652279 (250M) [text/csv]\n",
      "Saving to: ‘cit-Patents.csv’\n",
      "\n",
      "cit-Patents.csv     100%[===================>] 249.53M  74.1MB/s    in 3.7s    \n",
      "\n",
      "2024-09-26 23:32:10 (67.6 MB/s) - ‘cit-Patents.csv’ saved [261652279/261652279]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://downloadlink\"  # download datasets from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this\n",
    "dataset_folder = \"~/nvrliu/notebooks/demo/data/wikipedia\"\n",
    "\n",
    "edgelist_csv = f\"{dataset_folder}/enwiki-20240620-edges.csv\"\n",
    "nodedata_csv = f\"{dataset_folder}/enwiki-20240620-nodeids.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timed end-to-end code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Read the Wikipedia Connectivity data from `edgelist_csv`\n",
    "edgelist_df = pd.read_csv(\n",
    "    edgelist_csv,\n",
    "    sep=\" \",\n",
    "    names=[\"src\", \"dst\"],\n",
    "    dtype=\"int32\",\n",
    ")\n",
    "\n",
    "# Read the Wikipedia Page metadata from `nodedata_csv`\n",
    "nodedata_df = pd.read_csv(\n",
    "    nodedata_csv,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"nodeid\", \"title\"],\n",
    "    dtype={\"nodeid\": \"int32\", \"title\": \"str\"},\n",
    ")\n",
    "\n",
    "# Create a NetworkX graph from the connectivity info\n",
    "G = nx.from_pandas_edgelist(\n",
    "    edgelist_df,\n",
    "    source=\"src\",\n",
    "    target=\"dst\",\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "\n",
    "# Run pagerank on NetworkX\n",
    "nx_pr_vals = nx.pagerank(G)\n",
    "\n",
    "# Create a DataFrame containing the results\n",
    "pagerank_df = pd.DataFrame({\n",
    "    \"nodeid\": nx_pr_vals.keys(),\n",
    "    \"pagerank\": nx_pr_vals.values()\n",
    "})\n",
    "\n",
    "# Add NetworkX results to `nodedata` as new columns\n",
    "nodedata_df = nodedata_df.merge(pagerank_df, how=\"left\", on=\"nodeid\")\n",
    "\n",
    "# Here the top 25 pages based on pagerank value\n",
    "nodedata_df.sort_values(by=\"pagerank\", ascending=False).head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
